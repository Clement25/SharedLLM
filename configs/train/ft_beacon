--model_name_or_path /root/weihan/models/Llama-2-7b-chat-hf
--encoder_name_or_path /root/weihan/models/Llama-2-7b-chat-hf
--model_class sharedllm
--num_cross_attn_layers -16
--train_file data/activation-beacon/pretrain/redpajama-sample.json,data/activation-beacon/finetune/longalpaca.json
--do_train True
--train_encoder False
--eval_window 256
--do_eval False
--chunk_size 4096
--context_size 1024
--thr 0.0
--last_p "32,32"
--bf16 True
--torch_dtype bfloat16
--num_train_epochs 2
--init_mode none
--per_device_train_batch_size 1
--per_device_eval_batch_size 4
--gradient_accumulation_steps 1
--optim adamw_torch
--learning_rate 1e-5
--weight_decay 0
--warmup_ratio 0.001
--lr_scheduler_type cosine
--tf32 True
--max_length 8192
--min_length 1200
--max_train_num_per_data 200000
--save_strategy steps
--save_steps 8000
--save_total_limit 20
--logging_steps 50
--dataloader_num_workers 4
--ddp_find_unused_parameters True
--dataset_cache_dir /mnt/data/weihan/datasets/beacon-ft
--model_name_or_path /root/weihan/models/Llama-2-7b-hf
--encoder_name_or_path /root/weihan/models/Llama-2-7b-chat-hf
--num_cross_attn_layers -4
--train_file data/RP_sampled/rp_concat_8k/train
--train_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--do_train True
--train_encoder False
--do_eval False
--validation_file data/RP_sampled/rp_concat_8k/test
--validation_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--chunk_size 4096
--bf16 True
--model_class sharedllm
--num_context 4
--context_size 1024
--last_p "32,32"
--torch_dtype bfloat16
--init_mode none
--per_device_train_batch_size 1
--per_device_eval_batch_size 1
--gradient_accumulation_steps 16
--optim adamw_torch
--learning_rate 1e-5
--weight_decay 0
--warmup_ratio 0.001
--lr_scheduler_type cosine
--tf32 True
--evaluation_strategy steps
--eval_steps 20000
--save_strategy steps
--max_steps 20000
--save_steps 1000
--logging_steps 50
--save_total_limit 4
--dataloader_num_workers 4
--ddp_find_unused_parameters False
--kl_loss_cof 2
--lm_loss_cof 1
--kl_loss_mode drop

 